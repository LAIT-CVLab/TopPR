<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="TopP&R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>TopP&R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">TopP&R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              PumJun Kimsup>1</sup>,</span>
            <span class="author-block">
              Yoojin Jang<sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://jkim82133.github.io/">Jisu Kim</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.co.kr/citations?hl=en&user=7NBlQw4AAAAJ">Jaejun Yoo</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Ulsan National Institute of Science & Technology</span>
            <span class="author-block"><sup>2</sup>DataShape team, Inria Saclay, and LMO, Universit√© Paris-Saclay</span>
            <p><b>in NeurIPS 2023</b></p>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2306.08013"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2306.08013"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. 
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>-->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/LAIT-CVLab/TopPR"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. 
              <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>-->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>







<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We propose a robust and reliable evaluation metric for generative models 
            by introducing topological and statistical treatments for rigorous support estimation.
            </p>
            <p>
             Existing metrics, such as Inception Score (IS), Frechet Inception Distance (FID), 
             and the variants of Precision and Recall (P&R), heavily rely on supports that are
              estimated from sample features. However, the reliability of their estimation has 
              not been seriously discussed (and overlooked) even though the quality of the 
              evaluation entirely depends on it. 
            </p>
            <p>In this paper, we propose <b>Topological 
              Precision and Recall (TopP&R, pronounced '<span class="toppr" >topper</span>')</b>, which provides a systematic
               approach to estimating supports, retaining only topologically and statistically
                important features with a certain level of confidence. This not only makes TopP&R
                 strong for noisy features, but also provides statistical consistency. 
                 Our theoretical and experimental results show that TopP&R is robust to outliers
                  and non-independent and identically distributed (Non-IID) perturbations, 
                  while accurately capturing the true trend of change in samples. 
                  To the best of our knowledge, this is the first evaluation metric focused
                   on the robust estimation of the support and provides its statistical 
                   consistency under noise.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper overview -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
        <img src="./static/images/toppr_overview.png"
                 class="interpolation-image"
                 alt="Topological Precision and Recall"/>
            
      </div>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experiment for simultaneous mode dropping</h2>
        <h3 class="title is-4 has-text-justified">Toy dataset case</h3>
        <div class="content has-text-justified" >
        <p>Comparison of evaluation metrics for (a) sequential and (b) simultaneous mode-drop scenarios.
           The horizontal axis shows the concentration ratio on the distribution centered at 
           <img src="https://latex.codecogs.com/svg.latex?\mu=0"/>.</p>
      </div>
        <img src="./static/images/figure3.png"
                 class="interpolation-image"
                 alt="Simultaneous mode dropping"/>
            
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified" >
          <h3 class="title is-4 has-text-justified">Real dataset case</h3>
        <p>Comparison of evaluation metrics under sequential and simultaneous mode dropping scenario with Baby ImageNet.</p>
      </div>
        <img src="./static/images/figureA5.png"
                 class="interpolation-image"
                 alt="Simultaneous mode dropping"/>
            
      </div>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experiment for evaluating various generative models</h2>
        <div class="content has-text-justified" >
        <p>Generative models trained on CIFAR-10 are ranked by FID, KID and MTD, and F1-scores 
          based on TopP&R, D&C and P&R, respectively. The <img src="https://latex.codecogs.com/svg.latex?\mathcal{X} \; \text{and} \; \mathcal{Y}"> 
          are embedded with InceptionV3, VGG16, and SwAV. The number inside the parenthesis
           denotes the rank based on each metric.</p>
      </div>
        <img src="./static/images/table1.png"
                 class="interpolation-image"
                 alt="Simultaneous mode dropping"/>
            
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{kim2023topp,
      title={TopP$\backslash$\&R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models},
      author={Kim, Pum Jun and Jang, Yoojin and Kim, Jisu and Yoo, Jaejun},
      journal={arXiv preprint arXiv:2306.08013},
      year={2023}
    }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered is-centered">
      <p>Lab. of Advanced Imaging Tech</p>
      
      <a href="https://sites.google.com/view/jaejunyoo"><img
        src="static/images/others/logo_lait.png" width="50px"></a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            The website source code is from the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
