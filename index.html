<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="TopP&R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>TopP&R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">TopP&R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              PumJun Kim<sup>1</sup>,</span>
            <span class="author-block">
              Yoojin Jang<sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://jkim82133.github.io/">Jisu Kim</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.co.kr/citations?hl=en&user=7NBlQw4AAAAJ">Jaejun Yoo</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Ulsan National Institute of Science & Technology</span>
            <span class="author-block"><sup>2</sup>DataShape team, Inria Saclay, and LMO, Université Paris-Saclay</span>
            <p><b>in NeurIPS 2023</b></p>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2306.08013"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2306.08013"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. 
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>-->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/LAIT-CVLab/TopPR"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. 
              <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>-->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>







<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We propose a robust and reliable evaluation metric for generative models 
            by introducing topological and statistical treatments for rigorous support estimation.
            </p>
            <p>
             Existing metrics, such as Inception Score (IS), Frechet Inception Distance (FID), 
             and the variants of Precision and Recall (P&R), heavily rely on supports that are
              estimated from sample features. However, the reliability of their estimation has 
              not been seriously discussed (and overlooked) even though the quality of the 
              evaluation entirely depends on it. 
            </p>
            <p>In this paper, we propose <b>Topological 
              Precision and Recall (TopP&R, pronounced '<span class="toppr" >topper</span>')</b>, which provides a systematic
               approach to estimating supports, retaining only topologically and statistically
                important features with a certain level of confidence. This not only makes TopP&R
                 strong for noisy features, but also provides statistical consistency. 
                 Our theoretical and experimental results show that TopP&R is robust to outliers
                  and non-independent and identically distributed (Non-IID) perturbations, 
                  while accurately capturing the true trend of change in samples. 
                  To the best of our knowledge, this is the first evaluation metric focused
                   on the robust estimation of the support and provides its statistical 
                   consistency under noise.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper overview -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
        <img src="./static/images/toppr_overview.png"
                 class="interpolation-image"
                 alt="Topological Precision and Recall"/>
            
      </div>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experiments for mode dropping</h2>
        <h3 class="title is-4 has-text-justified">Toy dataset case</h3>
        <div class="content has-text-justified" >
        <p>
        We compare evaluation metrics for (a) sequential and (b) simultaneous mode-drop scenarios.
          The horizontal axis shows the concentration ratio on the distribution centered at 
          <img src="https://latex.codecogs.com/svg.latex?\mu=0"/>.
             We observe that the values of Precision fail to saturate, <img src="https://latex.codecogs.com/svg.latex? i.e.">, mainly smaller than 1, and the Density fluctuates to a value greater than 1, showing their instability and unboundedness. 
             Recall and GCA do not respond to the simultaneous mode drop, and Coverage decays slowly compared to the reference line. In contrast, TopP performs well, being held at the upper bound of 1 in sequential mode drop, 
             and TopR also decreases closest to the reference line in simultaneous mode drop.


          </p>
      </div>
        <img src="./static/images/figure3.png"
                 class="interpolation-image"
                 alt="Simultaneous mode dropping"/>
            
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified" >
          <h3 class="title is-4 has-text-justified">Real dataset case</h3>
        <p>
          We have conducted an additional experiment using Baby ImageNet
           to investigate the sensitivity of TopP&R to mode-drop in real-world data.
           Since our experiment involves gradually reducing the fixed number of fake
            samples until nine modes of the fake distribution vanish, the ground truth
             diversity should decrease linearly.
             From the experimental results, both D&C and P&R still struggle to respond to
               simultaneous mode dropping. In contrast, TopP&R consistently exhibit a high
                level of sensitivity to subtle distribution changes. This notable
                 capability of TopP&R can be attributed to its direct approximation of the
                  underlying distribution, distinguishing it from other metrics.
          
          </p>
      </div>
        <img src="./static/images/figureA5.png"
                 class="interpolation-image"
                 alt="Simultaneous mode dropping"/>
            
      </div>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experiment for ranking various generative models</h2>
        <div class="content has-text-justified" >
        <p>Generative models trained on CIFAR-10 are ranked by FID, KID and MTD, and F1-scores 
          based on TopP&R, D&C and P&R, respectively. The <img src="https://latex.codecogs.com/svg.latex?\mathcal{X} \; \text{and} \; \mathcal{Y}"> 
          are embedded with InceptionV3, VGG16, and SwAV. The number inside the parenthesis
           denotes the rank based on each metric.
          While other metrics exhibit fluctuating rankings, TopP&R consistently provides the most stable and consistent results similar to both FID and KID. 
          To quantitatively compare the similarity of rankings across varying embedders by different metrics, 
          we have computed mean Hamming Distance (MHD) where lower value indicates more similarity. 
          TopP&R, P&R, D&C, and MTD have MHDs of 1.33, 2.66, 3.0, and 3.33, respectively.
          </p>
      </div>
        <img src="./static/images/table1.png"
                 class="interpolation-image"
                 alt="Simultaneous mode dropping"/>
            
      </div>
    </div>
    <!--/ Paper video. -->
    <br>
    <br>

    <div class="columns is-centered ">
      <div class="column is-four-fifths">
        <div class="content has-text-justified" >
          <h2 class="title is-3 has-text-justified">References</h2>
          <ol>
        <li>Tuomas Kynkäänniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision
and recall metric for assessing generative models. Advances in Neural Information Processing Systems, 32,
2019.</li>
        <li>Muhammad Ferjad Naeem, Seong Joon Oh, Youngjung Uh, Yunjey Choi, and Jaejun Yoo. Reliable fidelity
and diversity metrics for generative models. In International Conference on Machine Learning, pages
7176–7185. PMLR, 2020.</li>
          <li>Minguk Kang, Joonghyuk Shin, and Jaesik Park. Studiogan: A taxonomy and benchmark of gans for
image synthesis. arXiv preprint arXiv:2206.09479, 2022.</li>
          <li>Serguei Barannikov, Ilya Trofimov, Grigorii Sotnikov, Ekaterina Trimbach, Alexander Korotin, Alexander
Filippov, and Evgeny Burnaev. Manifold topology divergence: a framework for comparing data manifolds.
Advances in Neural Information Processing Systems, 34, 2021.</li>
<li>Petra Poklukar, Anastasiia Varava, and Danica Kragic. Geomca: Geometric evaluation of data
  representations. In International Conference on Machine Learning, pages 8588–8598. PMLR, 2021.
  </li>
  <li>
    Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans
trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information
processing systems, 30, 2017.
  </li>
</ol>



      </div>
      </div>
    </div>

  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{kim2023topp,
      title={TopP$\backslash$\&R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models},
      author={Kim, Pum Jun and Jang, Yoojin and Kim, Jisu and Yoo, Jaejun},
      journal={arXiv preprint arXiv:2306.08013},
      year={2023}
    }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered is-centered">
      <p>Lab. of Advanced Imaging Tech</p>
      
      <a href="https://sites.google.com/view/jaejunyoo"><img
        src="static/images/others/logo_lait.png" width="50px"></a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            The website source code is from the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
